{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Tensorflow作用于推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 数据准备 - 以Movielens 为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"Movieslens 数据格式: user,item,rating,timestamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 数据预处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import,division,print_function => print (\"this python3.x \") 需要按照python3 语法规则\n",
    "#coding:utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\"function 1:\"\n",
    "def read_data_and_process(filename, sep = \"\\t\"):\n",
    "    col_names = [\"user\",\"item\",\"rate\",\"timestamp\"]\n",
    "    df        = pd.read_csv(filepath_or_buffer = filename, sep = sep, header = None, names = col_names, engine = 'python')\n",
    "    \"逐级递减\"\n",
    "    df['user'] -= 1 \n",
    "    df['item'] -= 1\n",
    "    for col in ('user','item'):\n",
    "        df[col]  = df[col].astype(np.int32)\n",
    "    \n",
    "    df['rate'] = df['rate'].astype(np.float32) #\"转换数据类型by astype\"\n",
    "    return df\n",
    "    \n",
    "\n",
    "\"随机生成一个batch一个batch的数据来保证每次训练模型拿到的数据均不同，避免过拟合的出现\"\n",
    "class ShuffleBatchData(object): # object \n",
    "    \n",
    "    def __init__ (self,inputs,batch_size = 10):\n",
    "        self.inputs     = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len      = len(self.inputs[0])\n",
    "        self.inputs   = np.transpose( np.vstack([np.array(self.inputs[i] for i in range(self.num_cols))] ) )\n",
    "        \n",
    "    \n",
    "    def __len__ (self):\n",
    "        return self.len\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    \"随机生成一个batch size个下标, 并取出相应的样本\"\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0,self.len,(self.batch_size, ) )\n",
    "        out = self.inputs[ids,:]\n",
    "        \n",
    "        return [out[:,columns_index] for columns_index in range(self.num_cols)]\n",
    "\n",
    "\"顺序产生一个epoch的数据,用于测试中....\"\n",
    "class OneEpochTestDataProcesing(ShuffleBatchData):\n",
    "    def __init__(self,inputs,batch_size = 10):\n",
    "        super(ShuffleBatchData,self).__init__(inputs,batch_size = batch_size)\n",
    "        if batch_size>0:\n",
    "            self.idx_group  = np.array_split(np.arange(self.len), np.ceil(self.len/batch_size) )\n",
    "        else:\n",
    "            self.idx_group  = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "        \n",
    "    def next(self):\n",
    "        if self.group_id > len(self.idx_group):\n",
    "            self.group_id = 0 \n",
    "            raise StopIteration \n",
    "        out = self.inputs[self.idx_group[self.group_id],:]\n",
    "        self.group_id += 1 \n",
    "        \n",
    "        return [out[:,i] for i in range(self.num_cols)]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 搭建model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\"使用矩阵分解搭建网络结构\"\n",
    "def interface_svd(user_batch,item_batch,user_num,item_num,dim = 5, device = \"/cpu:0\"):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        \"初始化几个偏执项\"\n",
    "        global_bias = tf.get_variable(\"global_bias\",shape = [])\n",
    "        w_bias_user = tf.get_variable('embd_bias_user',shape = [user_num])\n",
    "        w_bias_item = tf.get_variable('embd_bias_item',shape = [item_num])\n",
    "        \n",
    "        \"bias向量\"\n",
    "        bias_user = tf.nn.embedding_lookup(w_bias_user,user_batch,name = \"bias_user\")\n",
    "        bias_item = tf.nn.embedding_lookup(w_bias_item,item_batch,name = \"item_user\")\n",
    "        \n",
    "        w_user = tf.get_variable(\"embd_user\",shape = [user_num,dim], initializer = tf.truncated.normal_initializer(stddev = 0.02) )\n",
    "        w_item = tf.get_variable(\"embd_item\",shape = [item_num,dim], initializer = tf.truncated.normal_initializer(stddev = 0.02))\n",
    "        \n",
    "        \"user向量与item向量\"\n",
    "        embd_user = tf.nn.embedding_lookup(w_user,user_batch, name = \"embedding_user\")\n",
    "        embd_item = tf.nn.embedding_lookup(w_item,item_batch,name = \"embedding_item\")\n",
    "         \n",
    "        \"以上的都是tensorflow 规定的向量初始化过程\"\n",
    "        \n",
    "    with td.device(device):\n",
    "        \"按照公式对user 向量和item 向量求和\"\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user,embd_item),1)\n",
    "        \n",
    "        \"按照公式加上几个偏置项\"\n",
    "        infer = tf.add(infer,global_bias)\n",
    "        infer = tf.add(infer,bias_user)\n",
    "        infer = tf.add(infer,bias_item,name = 'svc_inference')\n",
    "        \n",
    "        \"加上正则化项\"\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user),tf.nn.l2_loss(embd_item),name = 'svd_regularizer')\n",
    "        return infer, regularizer\n",
    "\n",
    "\"模型迭代\"\n",
    "def optimization(infer,regularizer,rate_batch,learning_rate = 0.001,reg = 0.10,device = \"./cpu:0\"):\n",
    "    global_step = tf.train.get_global_step()\n",
    "    assert global_step is not None\n",
    "        \n",
    "    \"选择合适的optimizer 优化\"\n",
    "    with tf.device(device):\n",
    "        cost_l2  = tf.nn.l2_loss(tf.subtract(infer,rate_batch))\n",
    "        penalty  = tf.constant(reg,dtype = tf.float32, shape = [], name = 'l2')\n",
    "        cost     = tf.add(cost_l2, tf.multiply(regularizer,penalty))\n",
    "        train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step = global_step)\n",
    "    return cost,train_optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.在实际数据上训练模型 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six import next \n",
    "from tensorflow.core.framework import summary_pb2\n",
    "np.random.seed(123)\n",
    "\n",
    "\"模型常量定义\"\n",
    "BATCH_SIZE =  2000\n",
    "USER_NUM =  6040\n",
    "ITEM_NUM =  3952\n",
    "DIM  = 15 # \"factor 维度\"\n",
    "\n",
    "\"最大迭代轮数\"\n",
    "EPOCH_MAX = 200\n",
    "\n",
    "\"使用cpu做训练\"\n",
    "DEVICE = \"./cpu:0\"\n",
    " \n",
    "\"截断\"\n",
    "def clip(x):\n",
    "    return np.clip(x,1,5)\n",
    "\n",
    "\"方便可视化做的summary\"\n",
    "def make_scalar_summary(name,value):\n",
    "    return summary_pb2.Summary(value = [summary_pb2.Summary.Value(tag = name,simple_value = value )])\n",
    "\n",
    "\"通过调用上面定义的函数获取数据:\"\n",
    "def get_data():\n",
    "    df = read_data_and_process(\"./movielens/ml-1m/ratings.dat\",sep = \"::\")\n",
    "    rows = len(df)\n",
    "    \"permutation - 生成随机队列\"\n",
    "    df   = df.iloc[np.random.permutation(rows)].reset_index(drop = True)\n",
    "    split_index = int(rows * 0.9)\n",
    "    df_train    = df[0:split_index]\n",
    "    'reset_index : We can use the `drop = True` parameter to avoid the old index being added as a column'\n",
    "    df_test     = df[split_index:].reset_index(drop = True )\n",
    "    print(df_train.shape,df_test.shape)\n",
    "    return df_train,df_test\n",
    "\n",
    "\"实现训练过程\"\n",
    "def svd(train,test):\n",
    "    samples_per_batch = len(train) # \"batch size\"\n",
    "    \n",
    "    \"将数据一个batch一个batch喂到模型里面训练\"\n",
    "    iter_train = ShuffleBatchData([train[\"user\"],train[\"item\"], train[\"rate\"] ],batch_size = BATCH_SIZE)\n",
    "    \n",
    "    \"测试数据\"\n",
    "    iter_test  = OneEpochTestDataProcesing([test[\"user\"], test[\"item\"], test[\"rate\"]], batch_size = -1)\n",
    "    \n",
    "    \"user and item batch \"\n",
    "    user_batch = tf.placeholder(tf.int32,shape = [None], name = \"id_user\")\n",
    "    item_batch = tf.placeholder(tf.int32,shape = [None],name = \"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.int32,shape = [None])\n",
    "    \n",
    "    \"构建Graph 和训练\"\n",
    "    infer, regularizer  = interface_svd(user_batch,item_batch,USER_NUM,ITEM_NUM,DIM,DEVICE)\n",
    "    \n",
    "    global_step =  tf.contrib.get_or_create_global_step()\n",
    "    cost,train_optimizer  = optimization(infer,regularizer,rate_batch,learning_rate = 0.001,reg = 0.10,device = DEVICE)\n",
    "    \n",
    "    \"初始化所有变量\"\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    \"开始迭代\"\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        summary_writer = tf.summary.FileWriter(logdir = '/InternalData/log', graph = sess.graph )\n",
    "        print \"{} {} {} {}\".format(\"epoch\",\"train_error\",\"val_error\",\"elapsed_time\") \n",
    "        errors = deque(maxlen = samples_per_batch)\n",
    "        start  = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch):\n",
    "            users,items,rates = next(iter_train)\n",
    "            _,pred_batch = sess.run([train_optimizer,infer], feed_dict = {user_batch:users,\n",
    "                                                                         item_batch:items,\n",
    "                                                                         rate_batch:rates})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates,2) )\n",
    "            if i%samples_per_batch  == 0:\n",
    "                train_errors = np.sqrt(np.mean(errors))\n",
    "                test_errors  = np.array([])\n",
    "                \n",
    "                for users,items,rates in iter_test:\n",
    "                    pred_batch = sess.run(infer, feed_dict = {user_batch:users,\n",
    "                                                             item_batch:items})\n",
    "                    \n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    test_errors = np.append(test_errors,np.power(pred_batch - rates,2 ))\n",
    "                    \n",
    "                end  = time.time()\n",
    "                test_errors_sqrt = np.sqrt(np.mean(test_errors))\n",
    "                print \"{:3d} {:f} {:f} {:f}\".format(i//samples_per_batch,train_errors,test_errors_sqrt,end - start)\n",
    "                \n",
    "                train_err_summary = make_scalar_summary(\"training_error\",train_errors)\n",
    "                test_err_summary = make_scalar_summary(\"test_error\",test_errors_sqrt)\n",
    "                summary_writer.add_summary(train_err_summary,i)\n",
    "                summary_writer.add_summary(test_err_summary,i)\n",
    "                start = end \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"获取数据集\"\n",
    "train_datasets, test_datasets = get_data()\n",
    "\"SVD训练数据\"\n",
    "svd(train_datasets,test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename  = \"./RawData/ReleasaeFilesCRC12.5.csv\"\n",
    "col_names = [\"Baseline\",\"Date\"]\n",
    "DF   = pd.read_csv(filename, sep = \",\", header = None, names = col_names, engine = 'python')\n",
    "DF['Baseline'] =  1\n",
    "DF['Date'] = \"2019\"\n",
    "for col in (\"Baseline\",\"Date\"):\n",
    "    DF[col] =  DF[col].astype(np.float32)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method reset_index in module pandas.core.frame:\n",
      "\n",
      "reset_index(self, level=None, drop=False, inplace=False, col_level=0, col_fill='') unbound pandas.core.frame.DataFrame method\n",
      "    For DataFrame with multi-level index, return new DataFrame with\n",
      "    labeling information in the columns under the index names, defaulting\n",
      "    to 'level_0', 'level_1', etc. if any are None. For a standard index,\n",
      "    the index name will be used (if set), otherwise a default 'index' or\n",
      "    'level_0' (if 'index' is already taken) will be used.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    level : int, str, tuple, or list, default None\n",
      "        Only remove the given levels from the index. Removes all levels by\n",
      "        default\n",
      "    drop : boolean, default False\n",
      "        Do not try to insert index into dataframe columns. This resets\n",
      "        the index to the default integer index.\n",
      "    inplace : boolean, default False\n",
      "        Modify the DataFrame in place (do not create a new object)\n",
      "    col_level : int or str, default 0\n",
      "        If the columns have multiple levels, determines which level the\n",
      "        labels are inserted into. By default it is inserted into the first\n",
      "        level.\n",
      "    col_fill : object, default ''\n",
      "        If the columns have multiple levels, determines how the other\n",
      "        levels are named. If None then the index name is repeated.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    resetted : DataFrame\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame([('bird',    389.0),\n",
      "    ...                    ('bird',     24.0),\n",
      "    ...                    ('mammal',   80.5),\n",
      "    ...                    ('mammal', np.nan)],\n",
      "    ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n",
      "    ...                   columns=('class', 'max_speed'))\n",
      "    >>> df\n",
      "             class  max_speed\n",
      "    falcon    bird      389.0\n",
      "    parrot    bird       24.0\n",
      "    lion    mammal       80.5\n",
      "    monkey  mammal        NaN\n",
      "    \n",
      "    When we reset the index, the old index is added as a column, and a\n",
      "    new sequential index is used:\n",
      "    \n",
      "    >>> df.reset_index()\n",
      "        index   class  max_speed\n",
      "    0  falcon    bird      389.0\n",
      "    1  parrot    bird       24.0\n",
      "    2    lion  mammal       80.5\n",
      "    3  monkey  mammal        NaN\n",
      "    \n",
      "    We can use the `drop` parameter to avoid the old index being added as\n",
      "    a column:\n",
      "    \n",
      "    >>> df.reset_index(drop=True)\n",
      "        class  max_speed\n",
      "    0    bird      389.0\n",
      "    1    bird       24.0\n",
      "    2  mammal       80.5\n",
      "    3  mammal        NaN\n",
      "    \n",
      "    You can also use `reset_index` with `MultiIndex`.\n",
      "    \n",
      "    >>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n",
      "    ...                                    ('bird', 'parrot'),\n",
      "    ...                                    ('mammal', 'lion'),\n",
      "    ...                                    ('mammal', 'monkey')],\n",
      "    ...                                   names=['class', 'name'])\n",
      "    >>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n",
      "    ...                                      ('species', 'type')])\n",
      "    >>> df = pd.DataFrame([(389.0, 'fly'),\n",
      "    ...                    ( 24.0, 'fly'),\n",
      "    ...                    ( 80.5, 'run'),\n",
      "    ...                    (np.nan, 'jump')],\n",
      "    ...                   index=index,\n",
      "    ...                   columns=columns)\n",
      "    >>> df\n",
      "                   speed species\n",
      "                     max    type\n",
      "    class  name\n",
      "    bird   falcon  389.0     fly\n",
      "           parrot   24.0     fly\n",
      "    mammal lion     80.5     run\n",
      "           monkey    NaN    jump\n",
      "    \n",
      "    If the index has multiple levels, we can reset a subset of them:\n",
      "    \n",
      "    >>> df.reset_index(level='class')\n",
      "             class  speed species\n",
      "                      max    type\n",
      "    name\n",
      "    falcon    bird  389.0     fly\n",
      "    parrot    bird   24.0     fly\n",
      "    lion    mammal   80.5     run\n",
      "    monkey  mammal    NaN    jump\n",
      "    \n",
      "    If we are not dropping the index, by default, it is placed in the top\n",
      "    level. We can place it in another level:\n",
      "    \n",
      "    >>> df.reset_index(level='class', col_level=1)\n",
      "                    speed species\n",
      "             class    max    type\n",
      "    name\n",
      "    falcon    bird  389.0     fly\n",
      "    parrot    bird   24.0     fly\n",
      "    lion    mammal   80.5     run\n",
      "    monkey  mammal    NaN    jump\n",
      "    \n",
      "    When the index is inserted under another level, we can specify under\n",
      "    which one with the parameter `col_fill`:\n",
      "    \n",
      "    >>> df.reset_index(level='class', col_level=1, col_fill='species')\n",
      "                  species  speed species\n",
      "                    class    max    type\n",
      "    name\n",
      "    falcon           bird  389.0     fly\n",
      "    parrot           bird   24.0     fly\n",
      "    lion           mammal   80.5     run\n",
      "    monkey         mammal    NaN    jump\n",
      "    \n",
      "    If we specify a nonexistent level for `col_fill`, it is created:\n",
      "    \n",
      "    >>> df.reset_index(level='class', col_level=1, col_fill='genus')\n",
      "                    genus  speed species\n",
      "                    class    max    type\n",
      "    name\n",
      "    falcon           bird  389.0     fly\n",
      "    parrot           bird   24.0     fly\n",
      "    lion           mammal   80.5     run\n",
      "    monkey         mammal    NaN    jump\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.reset_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaselineNames</td>\n",
       "      <td>Date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CORE_RILC:CORE_RILC_FHM_API_00_002</td>\n",
       "      <td>1-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CORE_RILC:CORE_RILC_PDDAPI_00_001</td>\n",
       "      <td>2-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CORE_RILC:CORE_RILC_PDDDK_00_011</td>\n",
       "      <td>3-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CORE_RILC:CORE_RILC_PPCCBITI_01_002</td>\n",
       "      <td>4-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CORE_RILC:CORE_RILC_PPCFHM_01_001</td>\n",
       "      <td>5-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CORE_RILC:CORE_RILC_PPCFHMI_00_003</td>\n",
       "      <td>6-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CORE_RILC:CORE_RILC_PPCFSS_01_001</td>\n",
       "      <td>7-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CORE_RILC:CORE_RILC_PPCFSSI_00_003</td>\n",
       "      <td>8-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CORE_RILC:CORE_RILC_PPCMCCMS_00_003</td>\n",
       "      <td>9-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CORE_RILC:CORE_RILC_PPCPDDI_01_001</td>\n",
       "      <td>10-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CORE_RILC:CORE_RILC_PPCPDDR_01_002</td>\n",
       "      <td>11-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DEOS:DESK_ANSI_4_0_1</td>\n",
       "      <td>12-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DEOS:DESK_GCC_STARTUP_6_5_1</td>\n",
       "      <td>13-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DEOS:DESK_KERNEL_7_1_0_1</td>\n",
       "      <td>14-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DEOS:DESK_PPC_6_9_2</td>\n",
       "      <td>15-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DEOS:DESK_PRODUCE_7_0_0</td>\n",
       "      <td>16-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DEOS:DESK_PYTHON_3_0_0</td>\n",
       "      <td>17-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DEOS:DESK_VIDEO_6_4_1</td>\n",
       "      <td>18-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DEOS:MATH_PPC_0_9_3</td>\n",
       "      <td>19-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>EDS_PRD:EDS_PRD_PPCCIOBIT_03_002</td>\n",
       "      <td>20-Jan-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>EDS_PRD:EDS_PRD_PPCHAL_01_001</td>\n",
       "      <td>21-Jan-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Baseline       Date\n",
       "0                         BaselineNames       Date\n",
       "1    CORE_RILC:CORE_RILC_FHM_API_00_002   1-Jan-19\n",
       "2     CORE_RILC:CORE_RILC_PDDAPI_00_001   2-Jan-19\n",
       "3      CORE_RILC:CORE_RILC_PDDDK_00_011   3-Jan-19\n",
       "4   CORE_RILC:CORE_RILC_PPCCBITI_01_002   4-Jan-19\n",
       "5     CORE_RILC:CORE_RILC_PPCFHM_01_001   5-Jan-19\n",
       "6    CORE_RILC:CORE_RILC_PPCFHMI_00_003   6-Jan-19\n",
       "7     CORE_RILC:CORE_RILC_PPCFSS_01_001   7-Jan-19\n",
       "8    CORE_RILC:CORE_RILC_PPCFSSI_00_003   8-Jan-19\n",
       "9   CORE_RILC:CORE_RILC_PPCMCCMS_00_003   9-Jan-19\n",
       "10   CORE_RILC:CORE_RILC_PPCPDDI_01_001  10-Jan-19\n",
       "11   CORE_RILC:CORE_RILC_PPCPDDR_01_002  11-Jan-19\n",
       "12                 DEOS:DESK_ANSI_4_0_1  12-Jan-19\n",
       "13          DEOS:DESK_GCC_STARTUP_6_5_1  13-Jan-19\n",
       "14             DEOS:DESK_KERNEL_7_1_0_1  14-Jan-19\n",
       "15                  DEOS:DESK_PPC_6_9_2  15-Jan-19\n",
       "16              DEOS:DESK_PRODUCE_7_0_0  16-Jan-19\n",
       "17               DEOS:DESK_PYTHON_3_0_0  17-Jan-19\n",
       "18                DEOS:DESK_VIDEO_6_4_1  18-Jan-19\n",
       "19                  DEOS:MATH_PPC_0_9_3  19-Jan-19\n",
       "20     EDS_PRD:EDS_PRD_PPCCIOBIT_03_002  20-Jan-19\n",
       "21        EDS_PRD:EDS_PRD_PPCHAL_01_001  21-Jan-19"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
